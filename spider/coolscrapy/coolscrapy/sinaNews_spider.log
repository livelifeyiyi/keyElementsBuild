2017-05-07 11:47:32 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: coolscrapy)
2017-05-07 11:47:32 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'coolscrapy.spiders', 'SPIDER_MODULES': ['coolscrapy.spiders'], 'LOG_FILE': 'sinaNews_spider.log', 'BOT_NAME': 'coolscrapy'}
2017-05-07 11:48:27 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: coolscrapy)
2017-05-07 11:48:27 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'coolscrapy.spiders', 'SPIDER_MODULES': ['coolscrapy.spiders'], 'LOG_FILE': 'sinaNews_spider.log', 'BOT_NAME': 'coolscrapy'}
2017-05-07 11:48:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-05-07 11:48:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-05-07 11:48:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-05-07 11:48:33 [twisted] CRITICAL: Unhandled error in Deferred:
2017-05-07 11:48:33 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda\lib\site-packages\twisted\internet\defer.py", line 1299, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda\lib\site-packages\scrapy\crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "C:\Anaconda\lib\site-packages\scrapy\crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "C:\Anaconda\lib\site-packages\scrapy\crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Anaconda\lib\site-packages\scrapy\core\engine.py", line 70, in __init__
    self.scraper = Scraper(crawler)
  File "C:\Anaconda\lib\site-packages\scrapy\core\scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "C:\Anaconda\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Anaconda\lib\site-packages\scrapy\middleware.py", line 40, in from_settings
    mw = mwcls()
  File "C:\(O_O)!\bishe\code\my\spider\coolscrapy\coolscrapy\pipelines.py", line 44, in __init__
    create_news_table(engine)
  File "C:\(O_O)!\bishe\code\my\spider\coolscrapy\coolscrapy\models.py", line 18, in create_news_table
    Base.metadata.create_all(engine)
  File "C:\Anaconda\lib\site-packages\sqlalchemy\sql\schema.py", line 3622, in create_all
    tables=tables)
  File "C:\Anaconda\lib\site-packages\sqlalchemy\engine\base.py", line 1853, in _run_visitor
    with self._optional_conn_ctx_manager(connection) as conn:
  File "C:\Anaconda\lib\contextlib.py", line 17, in __enter__
    return self.gen.next()
  File "C:\Anaconda\lib\site-packages\sqlalchemy\engine\base.py", line 1846, in _optional_conn_ctx_manager
    with self.contextual_connect() as conn:
  File "C:\Anaconda\lib\site-packages\sqlalchemy\engine\base.py", line 2037, in contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "C:\Anaconda\lib\site-packages\sqlalchemy\engine\base.py", line 2076, in _wrap_pool_connect
    e, dialect, self)
  File "C:\Anaconda\lib\site-packages\sqlalchemy\engine\base.py", line 1405, in _handle_dbapi_exception_noconnection
    exc_info
  File "C:\Anaconda\lib\site-packages\sqlalchemy\util\compat.py", line 199, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb)
  File "C:\Anaconda\lib\site-packages\sqlalchemy\engine\base.py", line 2072, in _wrap_pool_connect
    return fn()
  File "C:\Anaconda\lib\site-packages\sqlalchemy\pool.py", line 376, in connect
    return _ConnectionFairy._checkout(self)
  File "C:\Anaconda\lib\site-packages\sqlalchemy\pool.py", line 708, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "C:\Anaconda\lib\site-packages\sqlalchemy\pool.py", line 480, in checkout
    rec = pool._do_get()
  File "C:\Anaconda\lib\site-packages\sqlalchemy\pool.py", line 1055, in _do_get
    self._dec_overflow()
  File "C:\Anaconda\lib\site-packages\sqlalchemy\util\langhelpers.py", line 60, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "C:\Anaconda\lib\site-packages\sqlalchemy\pool.py", line 1052, in _do_get
    return self._create_connection()
  File "C:\Anaconda\lib\site-packages\sqlalchemy\pool.py", line 323, in _create_connection
    return _ConnectionRecord(self)
  File "C:\Anaconda\lib\site-packages\sqlalchemy\pool.py", line 449, in __init__
    self.connection = self.__connect()
  File "C:\Anaconda\lib\site-packages\sqlalchemy\pool.py", line 602, in __connect
    connection = self.__pool._invoke_creator(self)
  File "C:\Anaconda\lib\site-packages\sqlalchemy\engine\strategies.py", line 97, in connect
    return dialect.connect(*cargs, **cparams)
  File "C:\Anaconda\lib\site-packages\sqlalchemy\engine\default.py", line 385, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "C:\Anaconda\lib\site-packages\MySQLdb\__init__.py", line 81, in Connect
    return Connection(*args, **kwargs)
  File "C:\Anaconda\lib\site-packages\MySQLdb\connections.py", line 187, in __init__
    super(Connection, self).__init__(*args, **kwargs2)
OperationalError: (_mysql_exceptions.OperationalError) (2003, "Can't connect to MySQL server on '127.0.0.1' (10061)")
